<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI and Mathematics Seminar</title>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&family=Roboto+Slab:wght@700&display=swap" rel="stylesheet">
  <style>
    /* Base */
    :root {
      --brand: #1a73e8;
      --ink: #333;
      --muted: #555;
      --bg: #f8f8f8;
      --card: #fff;
      --shadow: 0 4px 12px rgba(0,0,0,0.06);
      --radius: 10px;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: 'Open Sans', sans-serif;
      color: var(--ink);
      background: var(--bg);
      line-height: 1.7;
    }

    /* Hero */
    .hero {
      background: var(--brand);
      color: #fff;
      text-align: center;
      padding: 56px 20px 40px;
      border-radius: 0 0 12px 12px;
      box-shadow: var(--shadow);
    }
    .hero h1 {
      font-family: 'Roboto Slab', serif;
      font-weight: 700;
      margin: 0 0 12px;
      letter-spacing: .4px;
    }
    .hero p {
      margin: 0 auto;
      max-width: 780px;
      color: #eef3ff;
      font-size: 18px;
    }

    /* Container */
    .container {
      max-width: 900px;
      margin: 28px auto;
      background: var(--card);
      border-radius: var(--radius);
      box-shadow: var(--shadow);
      padding: 28px 22px;
    }

    /* Headings & text */
    h2, h3 {
      font-family: 'Roboto Slab', serif;
      color: var(--brand);
      margin: 18px 0 12px;
    }
    p { color: var(--muted); font-size: 18px; }

    /* Info bar */
    .meta {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 10px;
      background: #f4f9ff;
      border: 1px solid #e3efff;
      border-radius: 8px;
      padding: 12px 14px;
      margin: 14px 0 8px;
      font-size: 15px;
    }
    .meta b { color: #0c47b7; }

    /* Organizers row */
    .organizers {
      font-size: 16px;
      color: #444;
      margin-top: 8px;
      line-height: 1.6;
    }
    .organizers a {
      color: inherit;
      text-decoration: none;
      font-weight: 600;
    }
    .organizers a:hover {
      text-decoration: underline;
    }
    .organizers a + a::before {
      content: " · ";
      white-space: pre;
      font-weight: 400;
      color: #777;
    }

    /* Links */
    a { color: var(--brand); text-decoration: none; font-weight: 600; }
    a:hover { text-decoration: underline; }

    /* Table */
    .schedule-wrap { overflow-x: auto; }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 16px;
      min-width: 640px; /* better on small screens */
    }
    caption {
      caption-side: top;
      text-align: left;
      font-weight: 600;
      color: var(--ink);
      margin-bottom: 6px;
    }
    th, td {
      padding: 12px 10px;
      text-align: left;
      border-bottom: 1px solid #eaeaea;
      vertical-align: top;
    }
    th {
      background: #f6f6f6;
      font-weight: 700;
    }
    tbody tr:nth-child(even) { background: #fbfbfb; }

    /* Badge for TBA */
    .tba {
      display: inline-block;
      font-size: 12px;
      padding: 2px 8px;
      border-radius: 999px;
      background: #fff4cc;
      border: 1px solid #ffe08a;
      color: #7a5e00;
      margin-left: 6px;
      vertical-align: baseline;
    }

    /* Footer */
    footer {
      margin-top: 24px;
      font-size: 14px;
      color: #666;
    }

    /* Responsive tweaks */
    @media (max-width: 720px) {
      .hero { padding: 42px 18px 28px; }
      .hero h1 { font-size: 1.6rem; }
      .hero p { font-size: 16px; }
      p { font-size: 16px; }
    }
  </style>
</head>
<body>

  <!-- Hero -->
  <header class="hero" role="banner">
    <h1>AI and Mathematics Seminar</h1>
    <p>Weekly research seminar at Rutgers on the use of Lean and other AI methods in Mathematics research.
       <strong>Wednesdays, 10–11 AM</strong>.</p>
  </header>

  <!-- Main -->
  <main class="container" role="main">
    <section aria-labelledby="about">
      <h2 id="about">About</h2>

      <div class="meta" aria-label="Seminar logistics">
        <div><b>Time</b>: Wednesdays, <time datetime="10:00">10:00–11:00 AM</time></div>
        <div><b>Location</b>: Hill 705</div>
        <div><b>Mailing list</b>:
          <a href="mailto:ak2530@scarletmail.rutgers.edu">ak2530@scarletmail.rutgers.edu</a>
        </div>
      </div>

      <p>
        The seminar explores the fast-growing interface between mathematical reasoning, formal methods,
        and modern AI systems. It is co-organized by DIMACS and the Mathematics Department at Rutgers University, New Brunswick. <a href="https://www.youtube.com/@ai_and_math_seminar">Here</a> is the YouTube channel for the talks in this seminar.
      </p>

      <h3>Organizers</h3>
      <p class="organizers">
        <a href="https://sites.math.rutgers.edu/~carbonel/">Lisa Carbone</a>
        <a href="https://efirst.github.io">Emily First</a>
        <a href="http://ian.jauslin.org">Ian Jauslin</a>
        <a href="https://ayushkhaitanrutgers.github.io">Ayush Khaitan</a>
        <a href="https://sites.math.rutgers.edu/~alexk/">Alex Kontorovich</a>
        <a href="https://sites.math.rutgers.edu/~mischaik/">Konstantin Mischaikow</a>
        <a href="https://people.cs.rutgers.edu/~lirong.xia/">Lirong Xia</a>
      </p>
    </section>

    <section aria-labelledby="schedule">
      <h2 id="schedule">Seminar Schedule</h2>

      <!-- Fall 2025 (full info preserved, now collapsible) -->
      <details class="semester">
        <summary><strong>Fall 2025</strong></summary>
        <div class="schedule-wrap">
          <table aria-describedby="sched-note">
            <caption>Fall 2025</caption>
            <thead>
              <tr>
                <th scope="col">Date</th>
                <th scope="col">Speaker</th>
                <th scope="col">Title / Materials</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><time datetime="2025-10-01">Oct 1, 2025</time></td>
                <td>Alex Kontorovich (Rutgers)</td>
                <td>
                  <details class="talk">
                    <summary>
                      An Introduction to Lean + AI for Research Mathematicians
                    </summary>

                    <div class="talk-body">
                      <p><b>Abstract.</b> We'll do a "show and tell" of what it's like to try to formalize some basic mathematics in Lean, with help from AI.</p>

                      <ul class="materials">
                        <li><a href="https://rutgers.zoom.us/rec/play/MIF_HvQPSI21gnVKXEPnXq7C6PwPhCXAbATFYzNrlI7MjLDEpFceGm31RyQD0-EP68-aLwc6fwyPsMIC.z4k3PkG9k0uUKaxl?eagerLoadZvaPages=sidemenu.billing.plan_management&amp;accessLevel=meeting&amp;canPlayFromShare=true&amp;from=share_recording_detail&amp;continueMode=true&amp;componentName=rec-play&amp;originRequestUrl=https%3A%2F%2Frutgers.zoom.us%2Frec%2Fshare%2FQwH3qBna_ArS5V2fmepwBnMRMD1qDVuUWW0B0_LtCJcOg-8Ux5YKCgpYQMB-TsGa.Qvpw1sNnOU9NIwHu">Recording</a>, Passcode: g&amp;CcN6n=</li>
                      </ul>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-10-15">Oct 15, 2025</time></td>
                <td>Emily First (Rutgers)</td>
                <td>
                  <details class="talk">
                    <summary>A survey on AI for Proof Assistants</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> In this talk, I’ll provide an overview of some advancements in AI and machine learning in proof assistant languages, such as Lean, Rocq, and Isabelle/HOL. I’ll discuss both neural and symbolic techniques for various proof assistant tasks, including automated proof synthesis, autoformalization, premise selection, and lemma conjecturing. With respect to neural techniques, I’ll cover the basics of applying language models, reinforcement learning, and graph neural networks to the theorem proving domain.</p>
                      <ul class="materials">
                        <li>
                          <a href="https://rutgers.zoom.us/rec/share/LmP4iRzza6wbnfea--sPW5JFF-GjMr9zcA-ulPunSEEfkr4xAroIXYWuXFi76rrQ.OheCwT0BlSvCZG3Z?startTime=1760551188000">
                            Recording
                          </a>, Passcode: v8c%A!&amp;8
                        </li>
                      </ul>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-10-22">Oct 22, 2025</time></td>
                <td>Chi Jin (Princeton)</td>
                <td>
                  <details class="talk">
                    <summary>
                      Goedel-Prover-V2: The Strongest Open-Source Theorem Prover to Date
                    </summary>

                    <div class="talk-body">
                      <p><b>Abstract.</b> This talk introduces Goedel-Prover-V2, an open-source model that sets a new state of the art in automated theorem proving.</p>

                      <ul class="materials">
                      </ul>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-10-29">Oct 29, 2025</time></td>
                <td>Tomer Galanti (Texas A&amp;M)</td>
                <td>
                  <details class="talk">
                    <summary>LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> We seek algorithms for program learning that are both sample-efficient and computationally feasible. Classical results show that targets admitting short program descriptions (e.g., with short &quot;python code&quot;) can be learned with a &quot;small&quot; number of examples (scaling with the size of the code) via length-first program enumeration, but the search is exponential in description length. Consequently, Gradient-based training avoids this cost yet can require exponentially many samples on certain short-program families.</p>
                      <p>To address this gap, we introduce LLM-ERM, a propose-and-verify framework that replaces exhaustive enumeration with an LLM-guided search over candidate programs while retaining ERM-style selection on held-out data. Specifically, we draw k candidates with a pretrained reasoning-augmented LLM, compile and check each on the data, and return the best verified hypothesis, with no feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise online mini-batch SGD requires many samples to learn certain short programs. Empirically, LLM-ERM solves tasks such as parity variants, pattern matching, and primality testing with as few as 200 samples, while SGD-trained transformers overfit even with 100,000 samples. These results indicate that language-guided program synthesis recovers much of the statistical efficiency of finite-class ERM while remaining computationally tractable, offering a practical route to learning succinct hypotheses beyond the reach of gradient-based training.</p>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-11-05">Nov 5, 2025</time></td>
                <td>Ayush Khaitan (Rutgers)</td>
                <td>
                  <details class="talk">
                    <summary>O-Forge: a verifiable, LLM-driven framework for proving inequalities in research mathematics.</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> We introduce an LLM + computer software framework for proving sophisticated inequalities in research mathematics. We first ask a frontier LLM to break up a problem into its simplest parts, and then ask a computer algebra system to complete the proof for each part. In doing so, we solve a concrete question posed by Terence Tao. This is joint work with Vijay Ganesh (Georgia Tech).</p>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-11-12">Nov 12, 2025</time></td>
                <td>Chenyang An (Amazon)</td>
                <td>
                  <details class="talk">
                    <summary>DeRL: Diverse-Exploration Reinforcement Learning for Large Language Models</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> Current reinforcement-learning (RL) pipelines for large language models (LLMs) that tackle mathematical reasoning and formal theorem proving tend to over-exploit a few high-probability chain-of-thought (CoT) sequences. Because rewards are granted solely for producing correct answers, the policy quickly converges on those paths, neglecting the rich space of alternative proofs and solution strategies that math problems usually have. We address this limitation with Diverse-Exploration RL (DeRL), a simple yet effective modification to the reward function and the RL prompts. During training, the model is explicitly instructed to solve each problem without relying on its previously generated CoT. Next, an auxiliary LLM judge verifies the approach dissimilarity between the new LLM output and the previous CoT. Combined with the correctness metric, this new reward encourages exploration of novel reasoning paths while preserving accuracy. We test DeRL on both natural-language math questions with boxed answers and formal theorem proving problems in Lean. Across the MATH benchmark and Leanabell dataset, DeRL yields more than 10% relative gain compared to the PPO baseline for the Pass@1 metric. DeRL also consistently yields better results for the Pass@N metric. Our findings demonstrate that incorporating diversity-aware rewards facilitates broader exploration and enhances reasoning capabilities of LLMs, indicating a promising direction for improving current reinforcement learning pipelines.</p>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-11-19">Nov 19, 2025</time></td>
                <td>Vijay Ganesh (Georgia Tech)</td>
                <td>
                  <details class="talk">
                    <summary>Auto-formalization via Joint Embeddings</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> In recent years we have witnessed a symbiotic trend wherein LLMs are being combined with provers, solvers, and computer algebra systems, resulting in dramatic breakthroughs in AI for math. Following this trend, we have developed two lines of work in my research group. The first is the idea that &quot;good&quot; joint embeddings (JE) can dramatically improve the efficacy of LLM-based auto-formalization tools. We say that JEs are good if they respect the following invariant: semantically-equivalent formally-dissimilar objects (e.g., pairs of semantically-equivalent natural and formal language proofs) must be &quot;close by&quot; in the embedding space, and semantically inequivalent ones &quot;far apart&quot;. We use such JE models as part of a successful RAG-based auto-formalization pipeline, demonstrating that such JEs are a critical AI-for-math technology. The second idea is Reinforcement Learning with Symbolic Feedback (RLSF), a class of techniques that addresses the LLM hallucination problem in contexts where we have access to rich symbolic feedback such as math, physics, and code, demonstrating that they too are critical to the success of AI for math.</p>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-12-03">Dec 3, 2025</time></td>
                <td>Kaiyu Yang (FAIR, Meta)</td>
                <td>
                  <details class="talk">
                    <summary>Verina: Benchmarking Verifiable Code Generation</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation—jointly generating code, specifications, and proofs of code-specification alignment—offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often lack support for end-to-end verifiable code generation. In this paper, we introduce VERINA (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. VERINA consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs, with one trial per task. We hope VERINA will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark.</p>
                    </div>
                  </details>
                </td>
              </tr>

              <tr>
                <td><time datetime="2025-12-10">Dec 10, 2025</time></td>
                <td>Wuyang Chen (Simon Fraser University)</td>
                <td>
                  <details class="talk">
                    <summary>Hybrid Learning Machines Bridge AI and Physical Modeling</summary>
                    <div class="talk-body">
                      <p><b>Abstract.</b> Recent progress in LLMs has transformed text and code generation, yet models still falter on PDEs (partial differential equation) where correctness, constraints, and physical consequences are critical. This talk explores how formal LLM reasoning can advance symbolic PDE modeling. First, our PDE-Controller formalizes informal PDEs, synthesizes solver-ready code, and plans subgoals to tackle nonconvex control via interactions with external solvers. Second, our Lean Finder accelerates PDE formalization with a semantics-aware search engine for Lean/Mathlib that retrieves relevant theorems, outperforming GPT models and earning strong reception in the AI-for-math community. Together these efforts, our aim is to design a semantics-first LLM (large language model) that autoformalizes informal PDE problems into machine-checked specifications, synthesizes solver-ready code, and plans subgoals, closing the loop between formal analysis and LLM reasoning and surpassing human heuristics across diverse PDEs.</p>
                    </div>
                  </details>
                </td>
              </tr>

            </tbody>
          </table>
        </div>
      </details>

      <!-- Spring 2026 (dates only; speaker/title/materials empty) -->
      <details class="semester" open>
        <summary><strong>Spring 2026</strong></summary>
        <div class="schedule-wrap">
          <table aria-describedby="sched-note">
            <caption>Spring 2026</caption>
            <thead>
              <tr>
                <th scope="col">Date</th>
                <th scope="col">Speaker</th>
                <th scope="col">Title / Materials</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><time datetime="2026-01-28">Jan 28, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-02-04">Feb 4, 2026</time></td>
                <td>Kevin Buzzard (Imperial College) </td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-02-11">Feb 11, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-02-18">Feb 18, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-02-25">Feb 25, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-03-04">Mar 4, 2026</time></td>
                <td>Andrew Head (University of Pennsylvania) </td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-03-11">Mar 11, 2026</time></td>
                <td></td>
                <td></td>
              </tr>

              <!-- Spring Recess (no classes) includes Wed, Mar 18, 2026 -->

              <tr>
                <td><time datetime="2026-03-25">Mar 25, 2026</time></td>
                <td>Abhinav Verma (Penn State)</td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-04-01">Apr 1, 2026</time></td>
                <td>Gabriel Poesia (Harvard) </td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-04-08">Apr 8, 2026</time></td>
                <td>Eric Rodriguez (Harmonic)</td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-04-15">Apr 15, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-04-22">Apr 22, 2026</time></td>
                <td> Bennett Chow (UCSD) </td>
                <td></td>
              </tr>
              <tr>
                <td><time datetime="2026-04-29">Apr 29, 2026</time></td>
                <td></td>
                <td></td>
              </tr>
            </tbody>
          </table>
        </div>
      </details>

      <p id="sched-note" style="margin-top:8px; font-size:14px; color:#777;">
        Schedule subject to change; check back for abstracts and slides.
      </p>
    </section>

    <footer></footer>
  </main>
</body>
</html>
